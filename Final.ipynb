{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import chromedriver_binary\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url= 'https://mamikos.com/room/kost-kota-jakarta-pusat-kost-putra-eksklusif-kost-apik-kramat-kwitang-30-tipe-a-senen-jakarta-pusat?redirection_source=home%20promo%20ngebut'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Click web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#click unzoom\n",
    "unzoom= driver.find_element(By.CLASS_NAME,\"leaflet-control-zoom-out\")\n",
    "unzoom.click()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## new tab kos\n",
    "web= driver.find_elements(By.CLASS_NAME,\"kost-rc\")\n",
    "print(web)\n",
    "## looping throught each kos\n",
    "for i in web[0:1]:\n",
    "    print(i)\n",
    "    i.click()\n",
    "    time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## switch to new tab\n",
    "driver.switch_to.window(driver.window_handles[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## close new tab\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## back to main tab\n",
    "driver.switch_to.window(driver.window_handles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parsing with bs4\n",
    "soup = BeautifulSoup(driver.page_source,'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harga=soup.find('div',{'rc-price__real'}).get_text(strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Status kamar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status=soup.find('div',{'detail-kost-overview__availability-wrapper'}).get_text(strip=True)\n",
    "status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spesifikasi_tipe_kamar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spesifikasi_tipe_kamar=soup.find('div',{'detail-kost-room-specification'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_spesifikasi_tipe_kamar=spesifikasi_tipe_kamar.find_all('div','bg-c-list-item__description')\n",
    "for item in items_spesifikasi_tipe_kamar:\n",
    "    print(item.get_text(strip=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasilitas Kamar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kamar=soup.find('div',{'detail-kost-room-facilities'})\n",
    "items_kamar=kamar.find_all('div','bg-c-list-item__description')\n",
    "for item in items_kamar:\n",
    "    print(item.get_text(strip=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kamar Mandi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kamar_mandi=soup.find('div',{'detail-kost-bathroom-facilities'})\n",
    "items_kamar_mandi=kamar_mandi.find_all('div','bg-c-list-item__description')\n",
    "for item in items_kamar_mandi:\n",
    "    print(item.get_text(strip=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peraturan khusus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peraturan=soup.find('div',{'detail-kost-special-rules'})\n",
    "items_peraturan=peraturan.find_all('div','detail-kost-rule-item__content')\n",
    "for item in items_peraturan:\n",
    "    print(item.get_text(strip=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## special case\n",
    "try :\n",
    "    special_items_peraturan=peraturan.find_all('div','bg-c-list-item__right-content')\n",
    "    for item in special_items_peraturan:\n",
    "        print(item.get_text(strip=True))\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daerah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daerah=soup.find('p',{'detail-kost-overview__area-text bg-c-text bg-c-text--body-2'}).get_text(strip=True)\n",
    "daerah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasilitas Umum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umum=soup.find('div',{'detail-kost-public-facilities'})\n",
    "items_umum=umum.find_all('div','bg-c-list-item__description')\n",
    "for item in items_umum:\n",
    "    print(item.get_text(strip=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasilitas Parkir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parkir=soup.find('div',{'detail-kost-parking-facilities'})\n",
    "items_parkir=parkir.find_all('div','bg-c-list-item__description')\n",
    "for item in items_parkir:\n",
    "    print(item.get_text(strip=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peraturan Kos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peraturan=soup.find('div',{'detail-kost-rules__content'})\n",
    "items_peraturan=peraturan.find_all('div','bg-c-list-item__description')\n",
    "for item in items_peraturan:\n",
    "    print(item.get_text(strip=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## more content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loop=True\n",
    "while loop==True:\n",
    "    try :\n",
    "        print('klik more')\n",
    "        driver.find_element(By.CLASS_NAME,\"list__content-load-link\").click()\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        print('break')\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(10)\n",
    "        driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "        time.sleep(10)\n",
    "        loop=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ver 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import chromedriver_binary\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "\n",
    "def extract_item(soup,driver):\n",
    "    harga=[]\n",
    "    status=[]\n",
    "    daerah=[]\n",
    "    spec=[]\n",
    "    room=[]\n",
    "    broom=[]\n",
    "    srules=[]\n",
    "    facility=[]\n",
    "    parking=[]        \n",
    "    rules=[]\n",
    "    \n",
    "    url_tab = driver.current_url\n",
    "    name=soup.find('p',{'detail-title__room-name bg-c-text bg-c-text--heading-3'}).get_text(strip=True)\n",
    "    \n",
    "    try :\n",
    "\n",
    "        harga=soup.find('div',{'rc-price__real'}).get_text(strip=True)\n",
    "        \n",
    "        status=soup.find('div',{'detail-kost-overview__availability-wrapper'}).get_text(strip=True)\n",
    "\n",
    "        daerah=soup.find('p',{'detail-kost-overview__area-text bg-c-text bg-c-text--body-2'}).get_text(strip=True)\n",
    "               \n",
    "        spesifikasi=soup.find('div',{'detail-kost-room-specification'})\n",
    "        items_spesifikasi=spesifikasi.find_all('div','bg-c-list-item__description')\n",
    "        for item in items_spesifikasi:\n",
    "            spec.append(item.get_text(strip=True))\n",
    "                \n",
    "        kamar=soup.find('div',{'detail-kost-room-facilities'})\n",
    "        items_kamar=kamar.find_all('div','bg-c-list-item__description')\n",
    "        for item in items_kamar:\n",
    "            room.append(item.get_text(strip=True))\n",
    "        \n",
    "        kamar_mandi=soup.find('div',{'detail-kost-bathroom-facilities'})\n",
    "        items_kamar_mandi=kamar_mandi.find_all('div','bg-c-list-item__description')\n",
    "        for item in items_kamar_mandi:\n",
    "            broom.append(item.get_text(strip=True))\n",
    "               \n",
    "        peraturan=soup.find('div',{'detail-kost-special-rules'})\n",
    "        items_peraturan=peraturan.find_all('div','detail-kost-rule-item__content')\n",
    "        for item in items_peraturan:\n",
    "            srules.append(item.get_text(strip=True))\n",
    "        \n",
    "        try :\n",
    "            special_items_peraturan=peraturan.find_all('div','bg-c-list-item__right-content')\n",
    "            for item in special_items_peraturan:\n",
    "                srules.append(item.get_text(strip=True))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        \n",
    "        umum=soup.find('div',{'detail-kost-public-facilities'})\n",
    "        items_umum=umum.find_all('div','bg-c-list-item__description')\n",
    "        for item in items_umum:\n",
    "            facility.append(item.get_text(strip=True))\n",
    "\n",
    "        \n",
    "        parkir=soup.find('div',{'detail-kost-parking-facilities'})\n",
    "        items_parkir=parkir.find_all('div','bg-c-list-item__description')\n",
    "        for item in items_parkir:\n",
    "            parking.append(item.get_text(strip=True))\n",
    "        \n",
    "        peraturan=soup.find('div',{'detail-kost-rules__content'})\n",
    "        items_peraturan=peraturan.find_all('div','bg-c-list-item__description')\n",
    "        for item in items_peraturan:\n",
    "            rules.append(item.get_text(strip=True))\n",
    "    except:\n",
    "        pass\n",
    "    result = (name,url_tab,harga,status,daerah,spec,room,broom,srules,facility,parking,rules)\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def main(url):\n",
    "    \n",
    "    driver = webdriver.Chrome()\n",
    "    driver.maximize_window()\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    ## click unzoom\n",
    "    item=[]\n",
    "    unzoom= driver.find_element(By.CLASS_NAME,\"leaflet-control-zoom-out\")\n",
    "    unzoom.click()\n",
    "    time.sleep(10)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    ## click pop up\n",
    "    time.sleep(10)\n",
    "    driver.find_element(By.CLASS_NAME,'popper-ftue__content-button').click()\n",
    "    \n",
    "    \n",
    "    ## more content\n",
    "    loop=True\n",
    "    while loop==True:\n",
    "        try :\n",
    "            print('klik more')\n",
    "            driver.find_element(By.CLASS_NAME,\"list__content-load-link\").click()\n",
    "            time.sleep(5)\n",
    "        except:\n",
    "            print('break')\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(10)\n",
    "            driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "            time.sleep(10)\n",
    "            loop=False\n",
    "\n",
    "    ## new tab kos\n",
    "    print('newtab loop')\n",
    "\n",
    "    web=driver.find_elements(By.CLASS_NAME,\"kost-rc\")\n",
    "    print(len(web))\n",
    "    counting=0\n",
    "    \n",
    "    ## looping throught each kos and extract\n",
    "    for i in web:\n",
    "        print(counting)\n",
    "        try:\n",
    "            i.click()\n",
    "            time.sleep(20)\n",
    "            driver.switch_to.window(driver.window_handles[1])\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(10)\n",
    "            driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "            time.sleep(10)\n",
    "            soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "            \n",
    "            item.append(extract_item(soup,driver))\n",
    "\n",
    "            driver.close()\n",
    "            driver.switch_to.window(driver.window_handles[0])\n",
    "            counting=counting+1\n",
    "        except:\n",
    "            with open('Results.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(['name','url_tab','harga','status','daerah','spec','room','broom','srules','facility','parking','rules'])\n",
    "                writer.writerows(item)\n",
    "                \n",
    "            return driver,web,counting\n",
    "\n",
    "    with open('Results.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['name','url_tab','harga','status','daerah','spec','room','broom','srules','facility','parking','rules'])\n",
    "        writer.writerows(item)\n",
    "    \n",
    "    return driver,web,counting\n",
    "\n",
    "def continue_main (driver,web,counting):\n",
    "\n",
    "    for i in web[counting:]:\n",
    "        print(counting)\n",
    "        try:\n",
    "            i.click()\n",
    "            time.sleep(10)\n",
    "            driver.switch_to.window(driver.window_handles[1])\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(10)\n",
    "            driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "            time.sleep(10)\n",
    "            soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "            \n",
    "            item.append(extract_item(soup,driver))\n",
    "\n",
    "            driver.close()\n",
    "            driver.switch_to.window(driver.window_handles[0])\n",
    "            counting=counting+1\n",
    "\n",
    "        except:\n",
    "            with open('Results_continue.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(['name','url_tab','harga','status','daerah','spec','room','broom','srules','facility','parking','rules'])\n",
    "                writer.writerows(item)\n",
    "\n",
    "    with open('Results.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['name','url_tab','harga','status','daerah','spec','room','broom','srules','facility','parking','rules'])\n",
    "        writer.writerows(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ver 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import chromedriver_binary\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "\n",
    "def extract_item(soup):\n",
    "\n",
    "    name=soup.find('p',{'detail-title__room-name bg-c-text bg-c-text--heading-3'}).get_text(strip=True)\n",
    "    \n",
    "    harga=soup.find('div',{'rc-price__real'}).get_text(strip=True)\n",
    "\n",
    "    status=soup.find('div',{'detail-kost-overview__availability-wrapper'}).get_text(strip=True)\n",
    "\n",
    "    daerah=soup.find('p',{'detail-kost-overview__area-text bg-c-text bg-c-text--body-2'}).get_text(strip=True)\n",
    "\n",
    "    spec=[]\n",
    "    spesifikasi=soup.find('div',{'detail-kost-room-specification'})\n",
    "    items_spesifikasi=spesifikasi.find_all('div','bg-c-list-item__description')\n",
    "    for item in items_spesifikasi:\n",
    "        spec.append(item.get_text(strip=True))\n",
    "    \n",
    "    room=[]\n",
    "    kamar=soup.find('div',{'detail-kost-room-facilities'})\n",
    "    items_kamar=kamar.find_all('div','bg-c-list-item__description')\n",
    "    for item in items_kamar:\n",
    "        room.append(item.get_text(strip=True))\n",
    "\n",
    "    broom=[]\n",
    "    kamar_mandi=soup.find('div',{'detail-kost-bathroom-facilities'})\n",
    "    items_kamar_mandi=kamar_mandi.find_all('div','bg-c-list-item__description')\n",
    "    for item in items_kamar_mandi:\n",
    "        broom.append(item.get_text(strip=True))\n",
    "    \n",
    "    srules=[]\n",
    "    peraturan=soup.find('div',{'detail-kost-special-rules'})\n",
    "    items_peraturan=peraturan.find_all('div','detail-kost-rule-item__content')\n",
    "    for item in items_peraturan:\n",
    "        srules.append(item.get_text(strip=True))\n",
    "    \n",
    "    try :\n",
    "        special_items_peraturan=peraturan.find_all('div','bg-c-list-item__right-content')\n",
    "        for item in special_items_peraturan:\n",
    "            srules.append(item.get_text(strip=True))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    facility=[]\n",
    "    umum=soup.find('div',{'detail-kost-public-facilities'})\n",
    "    items_umum=umum.find_all('div','bg-c-list-item__description')\n",
    "    for item in items_umum:\n",
    "        facility.append(item.get_text(strip=True))\n",
    "\n",
    "    parking=[]\n",
    "    parkir=soup.find('div',{'detail-kost-public-facilities'})\n",
    "    items_umum=umum.find_all('div','bg-c-list-item__description')\n",
    "    for item in items_umum:\n",
    "        parking.append(item.get_text(strip=True))\n",
    "    \n",
    "    rules=[]\n",
    "    peraturan=soup.find('div',{'detail-kost-rules__content'})\n",
    "    items_peraturan=peraturan.find_all('div','bg-c-list-item__description')\n",
    "    for item in items_peraturan:\n",
    "        rules.append(item.get_text(strip=True))\n",
    "    \n",
    "    result = (harga,status,daerah,spec,room,broom,srules,facility,parking,rules)\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def main(url):\n",
    "    \n",
    "    driver = webdriver.Chrome()\n",
    "    driver.maximize_window()\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    ## click unzoom\n",
    "    unzoom= driver.find_element(By.CLASS_NAME,\"leaflet-control-zoom-out\")\n",
    "    unzoom.click()\n",
    "    time.sleep(10)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "    ## new tab kos\n",
    "    web= driver.find_elements(By.CLASS_NAME,\"kost-rc\")\n",
    "    time.sleep(10)\n",
    "    driver.find_element(By.CLASS_NAME,'popper-ftue__content-button').click()\n",
    "    \n",
    "    ## looping throught each kos and extract\n",
    "    for i in web:\n",
    "        i.click()\n",
    "        time.sleep(25)\n",
    "        \n",
    "        driver.switch_to.window(driver.window_handles[1])\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(10)\n",
    "        driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "        time.sleep(10)\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        item=[]\n",
    "        item.append(extract_item(soup))\n",
    "\n",
    "        driver.close()\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        \n",
    "\n",
    "    with open('Results.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['harga','status','daerah','spec','room','broom','srules','facility','parking','rules'])\n",
    "        writer.writerows(item)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ver 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import chromedriver_binary\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "\n",
    "def extract_item(soup,driver):\n",
    "    harga=[]\n",
    "    status=[]\n",
    "    daerah=[]\n",
    "    spec=[]\n",
    "    room=[]\n",
    "    broom=[]\n",
    "    srules=[]\n",
    "    facility=[]\n",
    "    parking=[]\n",
    "    rules=[]\n",
    "    \n",
    "    url_tab = driver.current_url\n",
    "    name=soup.find('p',{'detail-title__room-name bg-c-text bg-c-text--heading-3'}).get_text(strip=True)\n",
    "    \n",
    "    try :\n",
    "\n",
    "        harga=soup.find('div',{'rc-price__real'}).get_text(strip=True)\n",
    "\n",
    "        status=soup.find('div',{'detail-kost-overview__availability-wrapper'}).get_text(strip=True)\n",
    "\n",
    "        daerah=soup.find('p',{'detail-kost-overview__area-text bg-c-text bg-c-text--body-2'}).get_text(strip=True)\n",
    "        \n",
    "    \n",
    "        \n",
    "        spesifikasi=soup.find('div',{'detail-kost-room-specification'})\n",
    "        items_spesifikasi=spesifikasi.find_all('div','bg-c-list-item__description')\n",
    "        for item in items_spesifikasi:\n",
    "            spec.append(item.get_text(strip=True))\n",
    "        \n",
    "        \n",
    "        kamar=soup.find('div',{'detail-kost-room-facilities'})\n",
    "        items_kamar=kamar.find_all('div','bg-c-list-item__description')\n",
    "        for item in items_kamar:\n",
    "            room.append(item.get_text(strip=True))\n",
    "\n",
    "        \n",
    "        kamar_mandi=soup.find('div',{'detail-kost-bathroom-facilities'})\n",
    "        items_kamar_mandi=kamar_mandi.find_all('div','bg-c-list-item__description')\n",
    "        for item in items_kamar_mandi:\n",
    "            broom.append(item.get_text(strip=True))\n",
    "        \n",
    "        \n",
    "        peraturan=soup.find('div',{'detail-kost-special-rules'})\n",
    "        items_peraturan=peraturan.find_all('div','detail-kost-rule-item__content')\n",
    "        for item in items_peraturan:\n",
    "            srules.append(item.get_text(strip=True))\n",
    "        \n",
    "        try :\n",
    "            special_items_peraturan=peraturan.find_all('div','bg-c-list-item__right-content')\n",
    "            for item in special_items_peraturan:\n",
    "                srules.append(item.get_text(strip=True))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        \n",
    "        umum=soup.find('div',{'detail-kost-public-facilities'})\n",
    "        items_umum=umum.find_all('div','bg-c-list-item__description')\n",
    "        for item in items_umum:\n",
    "            facility.append(item.get_text(strip=True))\n",
    "\n",
    "        \n",
    "        parkir=soup.find('div',{'detail-kost-public-facilities'})\n",
    "        items_umum=umum.find_all('div','bg-c-list-item__description')\n",
    "        for item in items_umum:\n",
    "            parking.append(item.get_text(strip=True))\n",
    "        \n",
    "        \n",
    "        peraturan=soup.find('div',{'detail-kost-rules__content'})\n",
    "        items_peraturan=peraturan.find_all('div','bg-c-list-item__description')\n",
    "        for item in items_peraturan:\n",
    "            rules.append(item.get_text(strip=True))\n",
    "    except:\n",
    "        pass\n",
    "    result = (name,url_tab,harga,status,daerah,spec,room,broom,srules,facility,parking,rules)\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def main(url):\n",
    "    \n",
    "    driver = webdriver.Chrome()\n",
    "    driver.maximize_window()\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    ## click unzoom\n",
    "    item=[]\n",
    "    unzoom= driver.find_element(By.CLASS_NAME,\"leaflet-control-zoom-out\")\n",
    "    unzoom.click()\n",
    "    time.sleep(10)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    ## click pop up\n",
    "    time.sleep(10)\n",
    "    driver.find_element(By.CLASS_NAME,'popper-ftue__content-button').click()\n",
    "    \n",
    "    \n",
    "    ## more content\n",
    "    loop=True\n",
    "    while loop==True:\n",
    "        try :\n",
    "            print('klik more')\n",
    "            driver.find_element(By.CLASS_NAME,\"list__content-load-link\").click()\n",
    "            time.sleep(5)\n",
    "        except:\n",
    "            print('break')\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(10)\n",
    "            driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "            time.sleep(10)\n",
    "            loop=False\n",
    "\n",
    "    ## new tab kos\n",
    "    print('newtab loop')\n",
    "\n",
    "    web=driver.find_elements(By.CLASS_NAME,\"kost-rc\")\n",
    "    print(len(web))\n",
    "    counting=0\n",
    "    \n",
    "    ## looping throught each kos and extract\n",
    "    for i in range(0,len(web),5):\n",
    "        print(counting)\n",
    "        try:\n",
    "            #first tab\n",
    "            web[i].click()\n",
    "            time.sleep(2)\n",
    "            #second tab\n",
    "            web[i+1].click()\n",
    "            time.sleep(2)\n",
    "            \n",
    "            #third tab\n",
    "            web[i+2].click()\n",
    "            time.sleep(2)\n",
    "\n",
    "            #fourth tab\n",
    "            web[i+3].click()\n",
    "            time.sleep(2)\n",
    "            \n",
    "            #fifth tab\n",
    "            web[i+4].click()\n",
    "            time.sleep(2)\n",
    "\n",
    "            time.sleep(15)\n",
    "\n",
    "            #switch to 1\n",
    "            driver.switch_to.window(driver.window_handles[1])\n",
    "            time.sleep(5)\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            #switch to 2\n",
    "            driver.switch_to.window(driver.window_handles[2])\n",
    "            time.sleep(4)\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            \n",
    "            #switch to 3\n",
    "            driver.switch_to.window(driver.window_handles[3])\n",
    "            time.sleep(3)\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            #switch to 4\n",
    "            driver.switch_to.window(driver.window_handles[4])\n",
    "            time.sleep(3)\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            #switch to 5\n",
    "            driver.switch_to.window(driver.window_handles[5])\n",
    "            time.sleep(3)\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "\n",
    "\n",
    "            #switch to 1\n",
    "            driver.switch_to.window(driver.window_handles[5])\n",
    "            time.sleep(5)\n",
    "            driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "\n",
    "            #switch to 2\n",
    "            driver.switch_to.window(driver.window_handles[4])\n",
    "            time.sleep(4)\n",
    "            driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "            \n",
    "            #switch to 3\n",
    "            driver.switch_to.window(driver.window_handles[3])\n",
    "            time.sleep(4)\n",
    "            driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "\n",
    "            #switch to 4\n",
    "            driver.switch_to.window(driver.window_handles[2])\n",
    "            time.sleep(4)\n",
    "            driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "\n",
    "            #switch to 5\n",
    "            driver.switch_to.window(driver.window_handles[1])\n",
    "            time.sleep(4)\n",
    "            driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "                \n",
    "\n",
    "            #extract and close\n",
    "            driver.switch_to.window(driver.window_handles[5])\n",
    "            time.sleep(2)\n",
    "            soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "            item.append(extract_item(soup,driver))\n",
    "            time.sleep(3)\n",
    "            driver.close()\n",
    "            \n",
    "            driver.switch_to.window(driver.window_handles[4])\n",
    "            time.sleep(2)\n",
    "            soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "            item.append(extract_item(soup,driver))\n",
    "            time.sleep(3)\n",
    "            driver.close()\n",
    "\n",
    "            driver.switch_to.window(driver.window_handles[3])\n",
    "            time.sleep(2)\n",
    "            soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "            item.append(extract_item(soup,driver))\n",
    "            time.sleep(3)\n",
    "            driver.close()\n",
    "\n",
    "            driver.switch_to.window(driver.window_handles[2])\n",
    "            time.sleep(2)\n",
    "            soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "            item.append(extract_item(soup,driver))\n",
    "            time.sleep(3)\n",
    "            driver.close()\n",
    "            \n",
    "            driver.switch_to.window(driver.window_handles[1])\n",
    "            time.sleep(2)\n",
    "            soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "            item.append(extract_item(soup,driver))\n",
    "            time.sleep(3)\n",
    "            driver.close()\n",
    "            \n",
    "            \n",
    "            driver.switch_to.window(driver.window_handles[0])\n",
    "            counting=counting+5\n",
    "        except:\n",
    "            with open('Results.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(['name','url_tab','harga','status','daerah','spec','room','broom','srules','facility','parking','rules'])\n",
    "                writer.writerows(item)\n",
    "                \n",
    "            return driver,web,counting\n",
    "\n",
    "    with open('Results.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['name','url_tab','harga','status','daerah','spec','room','broom','srules','facility','parking','rules'])\n",
    "        writer.writerows(item)\n",
    "    \n",
    "    return driver,web,counting\n",
    "\n",
    "\n",
    "\n",
    "def continue_main (driver,web,counting):\n",
    "    item=[]\n",
    "\n",
    "    for i in range(counting,len(web),5):\n",
    "        print(counting)\n",
    "        try:\n",
    "            #first tab\n",
    "            web[i].click()\n",
    "            time.sleep(2)\n",
    "            #second tab\n",
    "            web[i+1].click()\n",
    "            time.sleep(2)\n",
    "            \n",
    "            #third tab\n",
    "            web[i+2].click()\n",
    "            time.sleep(2)\n",
    "\n",
    "            #fourth tab\n",
    "            web[i+3].click()\n",
    "            time.sleep(2)\n",
    "            \n",
    "            #fifth tab\n",
    "            web[i+4].click()\n",
    "            time.sleep(2)\n",
    "\n",
    "            time.sleep(15)\n",
    "\n",
    "            #switch to 1\n",
    "            driver.switch_to.window(driver.window_handles[1])\n",
    "            time.sleep(5)\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            #switch to 2\n",
    "            driver.switch_to.window(driver.window_handles[2])\n",
    "            time.sleep(4)\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            \n",
    "            #switch to 3\n",
    "            driver.switch_to.window(driver.window_handles[3])\n",
    "            time.sleep(3)\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            #switch to 4\n",
    "            driver.switch_to.window(driver.window_handles[4])\n",
    "            time.sleep(3)\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            #switch to 5\n",
    "            driver.switch_to.window(driver.window_handles[5])\n",
    "            time.sleep(3)\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "\n",
    "\n",
    "            #switch to 1\n",
    "            driver.switch_to.window(driver.window_handles[5])\n",
    "            time.sleep(5)\n",
    "            driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "\n",
    "            #switch to 2\n",
    "            driver.switch_to.window(driver.window_handles[4])\n",
    "            time.sleep(4)\n",
    "            driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "            \n",
    "            #switch to 3\n",
    "            driver.switch_to.window(driver.window_handles[3])\n",
    "            time.sleep(4)\n",
    "            driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "\n",
    "            #switch to 4\n",
    "            driver.switch_to.window(driver.window_handles[2])\n",
    "            time.sleep(4)\n",
    "            driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "\n",
    "            #switch to 5\n",
    "            driver.switch_to.window(driver.window_handles[1])\n",
    "            time.sleep(4)\n",
    "            driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "                \n",
    "\n",
    "            #extract and close\n",
    "            driver.switch_to.window(driver.window_handles[5])\n",
    "            time.sleep(2)\n",
    "            soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "            item.append(extract_item(soup,driver))\n",
    "            time.sleep(3)\n",
    "            driver.close()\n",
    "            \n",
    "            driver.switch_to.window(driver.window_handles[4])\n",
    "            time.sleep(2)\n",
    "            soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "            item.append(extract_item(soup,driver))\n",
    "            time.sleep(3)\n",
    "            driver.close()\n",
    "\n",
    "            driver.switch_to.window(driver.window_handles[3])\n",
    "            time.sleep(2)\n",
    "            soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "            item.append(extract_item(soup,driver))\n",
    "            time.sleep(3)\n",
    "            driver.close()\n",
    "\n",
    "            driver.switch_to.window(driver.window_handles[2])\n",
    "            time.sleep(2)\n",
    "            soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "            item.append(extract_item(soup,driver))\n",
    "            time.sleep(3)\n",
    "            driver.close()\n",
    "            \n",
    "            driver.switch_to.window(driver.window_handles[1])\n",
    "            time.sleep(2)\n",
    "            soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "            item.append(extract_item(soup,driver))\n",
    "            time.sleep(3)\n",
    "            driver.close()\n",
    "            \n",
    "            \n",
    "            driver.switch_to.window(driver.window_handles[0])\n",
    "            counting=counting+5\n",
    "        except:\n",
    "            with open('Results_continue.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(['name','url_tab','harga','status','daerah','spec','room','broom','srules','facility','parking','rules'])\n",
    "                writer.writerows(item)\n",
    "\n",
    "            return driver,web,counting  \n",
    "\n",
    "    with open('Results.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['name','url_tab','harga','status','daerah','spec','room','broom','srules','facility','parking','rules'])\n",
    "        writer.writerows(item)\n",
    "        \n",
    "    return driver,web,counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERSION 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import chromedriver_binary\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "Options.page_load_strategy='eager'\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def extract_link(soup,driver):\n",
    "\n",
    "    \n",
    "    url_tab = driver.current_url\n",
    "    name=soup.find('p',{'detail-title__room-name bg-c-text bg-c-text--heading-3'}).get_text(strip=True)\n",
    "\n",
    "    result = (name,url_tab)\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def extract_item(soup,driver):\n",
    "    harga=[]\n",
    "    status=[]\n",
    "    daerah=[]\n",
    "    spec=[]\n",
    "    room=[]\n",
    "    broom=[]\n",
    "    srules=[]\n",
    "    facility=[]\n",
    "    parking=[]\n",
    "    rules=[]\n",
    "    \n",
    "    url_tab = driver.current_url\n",
    "    name=soup.find('p',{'detail-title__room-name bg-c-text bg-c-text--heading-3'}).get_text(strip=True)\n",
    "    \n",
    "    try :\n",
    "\n",
    "        harga=soup.find('div',{'rc-price__real'}).get_text(strip=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try :\n",
    "        status=soup.find('div',{'detail-kost-overview__availability-wrapper'}).get_text(strip=True)\n",
    "\n",
    "    except :\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        daerah=soup.find('p',{'detail-kost-overview__area-text bg-c-text bg-c-text--body-2'}).get_text(strip=True)\n",
    "    except:\n",
    "        pass    \n",
    "\n",
    "    try:\n",
    "        spesifikasi=soup.find('div',{'detail-kost-room-specification'})\n",
    "        items_spesifikasi=spesifikasi.find_all('div','bg-c-list-item__description')\n",
    "        for item in items_spesifikasi:\n",
    "            spec.append(item.get_text(strip=True))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:    \n",
    "        kamar=soup.find('div',{'detail-kost-room-facilities'})\n",
    "        items_kamar=kamar.find_all('div','bg-c-list-item__description')\n",
    "        for item in items_kamar:\n",
    "            room.append(item.get_text(strip=True))\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    try:\n",
    "        kamar_mandi=soup.find('div',{'detail-kost-bathroom-facilities'})\n",
    "        items_kamar_mandi=kamar_mandi.find_all('div','bg-c-list-item__description')\n",
    "        for item in items_kamar_mandi:\n",
    "            broom.append(item.get_text(strip=True))\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    try:\n",
    "        peraturan=soup.find('div',{'detail-kost-special-rules'})\n",
    "        items_peraturan=peraturan.find_all('div','detail-kost-rule-item__content')\n",
    "        for item in items_peraturan:\n",
    "            srules.append(item.get_text(strip=True))\n",
    "    \n",
    "        try :\n",
    "            special_items_peraturan=peraturan.find_all('div','bg-c-list-item__right-content')\n",
    "            for item in special_items_peraturan:\n",
    "                srules.append(item.get_text(strip=True))\n",
    "        except:\n",
    "            pass\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    try:\n",
    "        umum=soup.find('div',{'detail-kost-public-facilities'})\n",
    "        items_umum=umum.find_all('div','bg-c-list-item__description')\n",
    "        for item in items_umum:\n",
    "            facility.append(item.get_text(strip=True))\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    try:\n",
    "        parkir=soup.find('div',{'detail-kost-parking-facilities'})\n",
    "        items_parkir=parkir.find_all('div','bg-c-list-item__description')\n",
    "        for item in items_parkir:\n",
    "            parking.append(item.get_text(strip=True))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        peraturan=soup.find('div',{'detail-kost-rules__content'})\n",
    "        items_peraturan=peraturan.find_all('div','bg-c-list-item__description')\n",
    "        for item in items_peraturan:\n",
    "            rules.append(item.get_text(strip=True))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    result = (name,url_tab,harga,status,daerah,spec,room,broom,srules,facility,parking,rules)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def main_link(url,counting=0,driver=False):\n",
    "    if driver== False:\n",
    "        driver = webdriver.Chrome()\n",
    "    else:\n",
    "        pass\n",
    "    driver.maximize_window()\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    ## click unzoom\n",
    "    unzoom= driver.find_element(By.CLASS_NAME,\"leaflet-control-zoom-out\")\n",
    "    unzoom.click()\n",
    "    time.sleep(10)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    ## click pop up\n",
    "    time.sleep(10)\n",
    "    driver.find_element(By.CLASS_NAME,'popper-ftue__content-button').click()\n",
    "\n",
    "    loop=True\n",
    "    while loop==True:\n",
    "        try :\n",
    "            print('klik more')\n",
    "            driver.find_element(By.CLASS_NAME,\"list__content-load-link\").click()\n",
    "            time.sleep(5)\n",
    "        except:\n",
    "            print('break')\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(10)\n",
    "            driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "            time.sleep(10)\n",
    "            loop=False\n",
    "    ## new tab kos\n",
    "    print('newtab loop')\n",
    "    driver.switch_to.window(driver.window_handles[0])\n",
    "    web=driver.find_elements(By.CLASS_NAME,\"col-12\")\n",
    "    print(len(web))\n",
    "\n",
    "    ## looping throught each kos and extract\n",
    "    item= []\n",
    "    for i in range(counting,len(web)):\n",
    "        print(counting)\n",
    "        try:\n",
    "            driver.switch_to.window(driver.window_handles[0])\n",
    "            web[i].click() \n",
    "            time.sleep(3)\n",
    "            \n",
    "            #extract and close\n",
    "            driver.switch_to.window(driver.window_handles[1])\n",
    "            soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "            \n",
    "            item.append(extract_link(soup,driver))\n",
    "            driver.close()\n",
    "            counting=counting+1\n",
    "            \n",
    "        except:\n",
    "            with open('nama.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(['name','url_tab'])\n",
    "                writer.writerows(item)\n",
    "            return driver,web,counting\n",
    "\n",
    "    with open('nama.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['name','url_tab'])\n",
    "        writer.writerows(item)\n",
    "    return driver,web,counting\n",
    "\n",
    "def main_extract(driver,url,counting=0):\n",
    "    print(len(url))\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.maximize_window()\n",
    "    item=[]\n",
    "\n",
    "    ## looping throught each kos and extract\n",
    "    for i in range(counting,len(url)):\n",
    "        print(counting)\n",
    "        try:\n",
    "            driver.get(url[i])\n",
    "            time.sleep(2)\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(3)\n",
    "            driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "\n",
    "            time.sleep(2)\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(3)\n",
    "            driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "\n",
    "            #extract and close\n",
    "            soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "            item.append(extract_item(soup,driver))\n",
    "            counting=counting+1\n",
    "            \n",
    "        except:\n",
    "            with open('hasil.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(['name','url_tab','harga','status','daerah','spec','room','broom','srules','facility','parking','rules'])\n",
    "                writer.writerows(item)\n",
    "            return driver,web,counting\n",
    "\n",
    "    with open('hasil.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['name','url_tab','harga','status','daerah','spec','room','broom','srules','facility','parking','rules'])\n",
    "        writer.writerows(item)\n",
    "    return driver,web,counting\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scrape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63fad39c3e307bb7c91cd52b343120cf05a71943cd9d8bb1dee6446ce0812e52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
